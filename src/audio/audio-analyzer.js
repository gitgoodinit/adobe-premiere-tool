/**
 * Audio Analyzer - Advanced Multi-Track Audio Analysis
 * Implements silence detection, auto-trimming, overlap detection, and dynamic ducking
 * Uses Web Audio API, FFmpeg integration, and machine learning techniques
 */

class AudioAnalyzer {
    constructor(audioContext) {
        this.audioContext = audioContext;
        this.sampleRate = audioContext.sampleRate;
        
        // Analysis configuration
        this.config = {
            silenceThreshold: -40, // dB
            silenceDuration: 0.5,  // seconds
            overlapThreshold: 0.3, // correlation threshold
            duckingRatio: 0.3,     // ducking amount
            fftSize: 2048,
            hopSize: 512,
            windowType: 'hanning'
        };
        
        // Analysis state
        this.isAnalyzing = false;
        this.analyzeId = null;
        
        // Track data storage
        this.trackData = new Map();
        this.analysisResults = {
            silence: [],
            overlaps: [],
            levels: [],
            spectral: []
        };
        
        // Web Audio nodes
        this.analyserNodes = new Map();
        this.scriptProcessors = new Map();
        
        // FFmpeg integration
        this.ffmpegPath = '/opt/homebrew/bin/ffmpeg';
        
        // Machine learning models (simplified)
        this.mlModels = {
            speechDetection: null,
            musicDetection: null,
            overlapClassifier: null
        };
        
        // Event handlers
        this.eventHandlers = {
            'silenceDetected': [],
            'overlapDetected': [],
            'levelChange': [],
            'analysisComplete': [],
            'duckingTriggered': []
        };
        
        // Initialize analysis components
        this.initializeAnalyzer();
    }
    
    async initializeAnalyzer() {
        console.log('Initializing audio analyzer...');
        
        // Initialize ML models (mock initialization)
        await this.initializeMLModels();
        
        // Set up analysis workers
        this.setupAnalysisWorkers();
        
        console.log('Audio analyzer initialized');
    }
    
    async initializeMLModels() {
        // In a real implementation, these would load actual ML models
        // For now, we'll use simplified detection algorithms
        
        this.mlModels.speechDetection = {
            predict: (spectralData) => {
                // Simple speech detection based on spectral characteristics
                const speechRange = spectralData.slice(50, 200); // ~300-3400Hz range
                const energy = speechRange.reduce((sum, val) => sum + val, 0);
                return energy > 0.1 ? 0.8 : 0.2; // confidence score
            }
        };
        
        this.mlModels.musicDetection = {
            predict: (spectralData) => {
                // Simple music detection based on harmonic content
                const harmonicPeaks = this.findHarmonicPeaks(spectralData);
                return harmonicPeaks.length > 3 ? 0.9 : 0.3;
            }
        };
        
        this.mlModels.overlapClassifier = {
            predict: (crossCorrelation, spectralSimilarity) => {
                const score = (crossCorrelation * 0.6) + (spectralSimilarity * 0.4);
                return score > this.config.overlapThreshold ? 0.85 : 0.15;
            }
        };
        
        console.log('ML models initialized');
    }
    
    setupAnalysisWorkers() {
        // Set up web workers for heavy analysis tasks
        // For now, we'll use the main thread with optimized algorithms
        this.analysisBuffer = new Float32Array(this.config.fftSize);
        this.fftBuffer = new Float32Array(this.config.fftSize);
        this.window = this.createWindow(this.config.fftSize, this.config.windowType);
    }
    
    // Track Management
    addTrack(trackId, audioBuffer) {
        const trackData = {\n            id: trackId,\n            buffer: audioBuffer,\n            duration: audioBuffer.duration,\n            sampleRate: audioBuffer.sampleRate,\n            numberOfChannels: audioBuffer.numberOfChannels,\n            analysisData: {\n                levels: [],\n                spectral: [],\n                silence: [],\n                features: {}\n            },\n            lastAnalysis: null\n        };\n        \n        this.trackData.set(trackId, trackData);\n        \n        // Create analyser node for real-time analysis\n        const analyser = this.audioContext.createAnalyser();\n        analyser.fftSize = this.config.fftSize;\n        analyser.smoothingTimeConstant = 0.8;\n        this.analyserNodes.set(trackId, analyser);\n        \n        console.log(`Track ${trackId} added for analysis`);\n        return true;\n    }\n    \n    removeTrack(trackId) {\n        this.trackData.delete(trackId);\n        \n        if (this.analyserNodes.has(trackId)) {\n            this.analyserNodes.get(trackId).disconnect();\n            this.analyserNodes.delete(trackId);\n        }\n        \n        if (this.scriptProcessors.has(trackId)) {\n            this.scriptProcessors.get(trackId).disconnect();\n            this.scriptProcessors.delete(trackId);\n        }\n        \n        return true;\n    }\n    \n    // Silence Detection\n    async detectSilence(trackId, options = {}) {\n        const config = { ...this.config, ...options };\n        const trackData = this.trackData.get(trackId);\n        \n        if (!trackData) {\n            throw new Error(`Track ${trackId} not found`);\n        }\n        \n        console.log(`Detecting silence in track ${trackId}...`);\n        \n        // Method 1: Web Audio API Analysis\n        const webAudioSilences = await this.detectSilenceWebAudio(trackData, config);\n        \n        // Method 2: FFmpeg Analysis (if available)\n        const ffmpegSilences = await this.detectSilenceFFmpeg(trackData, config);\n        \n        // Method 3: Time-domain analysis\n        const timeDomainSilences = this.detectSilenceTimeDomain(trackData, config);\n        \n        // Combine and validate results\n        const silenceRegions = this.combineSilenceResults([\n            webAudioSilences,\n            ffmpegSilences,\n            timeDomainSilences\n        ], config);\n        \n        // Store results\n        trackData.analysisData.silence = silenceRegions;\n        trackData.lastAnalysis = Date.now();\n        \n        // Emit events\n        this.emit('silenceDetected', {\n            trackId,\n            silenceRegions,\n            method: 'combined',\n            confidence: this.calculateSilenceConfidence(silenceRegions)\n        });\n        \n        return silenceRegions;\n    }\n    \n    async detectSilenceWebAudio(trackData, config) {\n        const buffer = trackData.buffer;\n        const channelData = buffer.getChannelData(0); // Use first channel\n        const silenceRegions = [];\n        \n        const windowSize = Math.floor(config.silenceDuration * buffer.sampleRate);\n        const stepSize = Math.floor(windowSize / 4); // 75% overlap\n        \n        let silenceStart = null;\n        \n        for (let i = 0; i < channelData.length - windowSize; i += stepSize) {\n            const window = channelData.slice(i, i + windowSize);\n            const rms = this.calculateRMS(window);\n            const dbLevel = rms > 0 ? 20 * Math.log10(rms) : -Infinity;\n            \n            const isSilent = dbLevel < config.silenceThreshold;\n            \n            if (isSilent && silenceStart === null) {\n                silenceStart = i / buffer.sampleRate;\n            } else if (!isSilent && silenceStart !== null) {\n                const silenceEnd = i / buffer.sampleRate;\n                const duration = silenceEnd - silenceStart;\n                \n                if (duration >= config.silenceDuration) {\n                    silenceRegions.push({\n                        start: silenceStart,\n                        end: silenceEnd,\n                        duration: duration,\n                        confidence: 0.8,\n                        method: 'webaudio'\n                    });\n                }\n                silenceStart = null;\n            }\n        }\n        \n        // Handle silence at the end\n        if (silenceStart !== null) {\n            const silenceEnd = buffer.duration;\n            const duration = silenceEnd - silenceStart;\n            \n            if (duration >= config.silenceDuration) {\n                silenceRegions.push({\n                    start: silenceStart,\n                    end: silenceEnd,\n                    duration: duration,\n                    confidence: 0.8,\n                    method: 'webaudio'\n                });\n            }\n        }\n        \n        return silenceRegions;\n    }\n    \n    async detectSilenceFFmpeg(trackData, config) {\n        // This would integrate with FFmpeg's silencedetect filter\n        // For now, return empty array as placeholder\n        console.log('FFmpeg silence detection would run here');\n        return [];\n    }\n    \n    detectSilenceTimeDomain(trackData, config) {\n        const buffer = trackData.buffer;\n        const channelData = buffer.getChannelData(0);\n        const silenceRegions = [];\n        \n        const windowSize = 1024;\n        let silenceStart = null;\n        let silentSamples = 0;\n        \n        for (let i = 0; i < channelData.length; i += windowSize) {\n            const end = Math.min(i + windowSize, channelData.length);\n            const window = channelData.slice(i, end);\n            \n            // Calculate zero-crossing rate\n            const zcr = this.calculateZeroCrossingRate(window);\n            \n            // Calculate spectral centroid\n            const spectralCentroid = this.calculateSpectralCentroid(window);\n            \n            // Combine metrics for silence detection\n            const energy = this.calculateRMS(window);\n            const dbLevel = energy > 0 ? 20 * Math.log10(energy) : -Infinity;\n            \n            const isSilent = (dbLevel < config.silenceThreshold) && \n                           (zcr < 0.1) && \n                           (spectralCentroid < 1000);\n            \n            if (isSilent) {\n                if (silenceStart === null) {\n                    silenceStart = i / buffer.sampleRate;\n                }\n                silentSamples += windowSize;\n            } else {\n                if (silenceStart !== null) {\n                    const duration = silentSamples / buffer.sampleRate;\n                    if (duration >= config.silenceDuration) {\n                        silenceRegions.push({\n                            start: silenceStart,\n                            end: silenceStart + duration,\n                            duration: duration,\n                            confidence: 0.7,\n                            method: 'timedomain'\n                        });\n                    }\n                }\n                silenceStart = null;\n                silentSamples = 0;\n            }\n        }\n        \n        return silenceRegions;\n    }\n    \n    combineSilenceResults(resultArrays, config) {\n        const allRegions = resultArrays.flat();\n        if (allRegions.length === 0) return [];\n        \n        // Sort by start time\n        allRegions.sort((a, b) => a.start - b.start);\n        \n        // Merge overlapping regions\n        const mergedRegions = [];\n        let currentRegion = allRegions[0];\n        \n        for (let i = 1; i < allRegions.length; i++) {\n            const region = allRegions[i];\n            \n            if (region.start <= currentRegion.end + 0.1) { // 100ms tolerance\n                // Merge regions\n                currentRegion.end = Math.max(currentRegion.end, region.end);\n                currentRegion.duration = currentRegion.end - currentRegion.start;\n                currentRegion.confidence = Math.max(currentRegion.confidence, region.confidence);\n            } else {\n                mergedRegions.push(currentRegion);\n                currentRegion = region;\n            }\n        }\n        mergedRegions.push(currentRegion);\n        \n        // Filter by minimum duration\n        return mergedRegions.filter(region => \n            region.duration >= config.silenceDuration\n        );\n    }\n    \n    // Auto-Trimming\n    calculateAutoTrimPoints(trackId, options = {}) {\n        const trackData = this.trackData.get(trackId);\n        if (!trackData) {\n            throw new Error(`Track ${trackId} not found`);\n        }\n        \n        const silenceRegions = trackData.analysisData.silence || [];\n        const config = { ...this.config, ...options };\n        \n        let trimIn = 0;\n        let trimOut = trackData.duration;\n        \n        // Find trim in point (first non-silent audio)\n        const firstSilence = silenceRegions.find(region => region.start === 0);\n        if (firstSilence) {\n            trimIn = firstSilence.end;\n        }\n        \n        // Find trim out point (last non-silent audio)\n        const lastSilence = silenceRegions.find(region => \n            region.end >= trackData.duration - 0.1\n        );\n        if (lastSilence) {\n            trimOut = lastSilence.start;\n        }\n        \n        // Add padding\n        const padding = options.padding || 0.1; // 100ms padding\n        trimIn = Math.max(0, trimIn - padding);\n        trimOut = Math.min(trackData.duration, trimOut + padding);\n        \n        const trimPoints = {\n            trimIn,\n            trimOut,\n            originalDuration: trackData.duration,\n            newDuration: trimOut - trimIn,\n            silenceRemoved: silenceRegions.reduce((total, region) => \n                total + region.duration, 0\n            )\n        };\n        \n        console.log(`Auto-trim points for track ${trackId}:`, trimPoints);\n        return trimPoints;\n    }\n    \n    // Overlap Detection\n    async detectOverlaps(trackIds, options = {}) {\n        if (trackIds.length < 2) {\n            return [];\n        }\n        \n        console.log(`Detecting overlaps between ${trackIds.length} tracks...`);\n        const config = { ...this.config, ...options };\n        const overlaps = [];\n        \n        // Compare each pair of tracks\n        for (let i = 0; i < trackIds.length; i++) {\n            for (let j = i + 1; j < trackIds.length; j++) {\n                const track1Id = trackIds[i];\n                const track2Id = trackIds[j];\n                \n                const pairOverlaps = await this.detectPairOverlap(\n                    track1Id, track2Id, config\n                );\n                \n                overlaps.push(...pairOverlaps);\n            }\n        }\n        \n        // Store results\n        this.analysisResults.overlaps = overlaps;\n        \n        this.emit('overlapDetected', {\n            overlaps,\n            trackCount: trackIds.length,\n            method: 'spectral-correlation'\n        });\n        \n        return overlaps;\n    }\n    \n    async detectPairOverlap(track1Id, track2Id, config) {\n        const track1Data = this.trackData.get(track1Id);\n        const track2Data = this.trackData.get(track2Id);\n        \n        if (!track1Data || !track2Data) {\n            return [];\n        }\n        \n        const overlaps = [];\n        const windowSize = Math.floor(config.fftSize / 2);\n        const hopSize = Math.floor(windowSize / 4);\n        \n        const buffer1 = track1Data.buffer.getChannelData(0);\n        const buffer2 = track2Data.buffer.getChannelData(0);\n        \n        const minLength = Math.min(buffer1.length, buffer2.length);\n        \n        for (let i = 0; i < minLength - windowSize; i += hopSize) {\n            const window1 = buffer1.slice(i, i + windowSize);\n            const window2 = buffer2.slice(i, i + windowSize);\n            \n            // Calculate cross-correlation\n            const crossCorr = this.calculateCrossCorrelation(window1, window2);\n            \n            // Calculate spectral similarity\n            const spectralSim = this.calculateSpectralSimilarity(window1, window2);\n            \n            // Use ML model to classify overlap\n            const overlapConfidence = this.mlModels.overlapClassifier.predict(\n                crossCorr, spectralSim\n            );\n            \n            if (overlapConfidence > 0.7) {\n                const timeStart = i / track1Data.buffer.sampleRate;\n                const timeEnd = (i + windowSize) / track1Data.buffer.sampleRate;\n                \n                overlaps.push({\n                    track1: track1Id,\n                    track2: track2Id,\n                    startTime: timeStart,\n                    endTime: timeEnd,\n                    duration: timeEnd - timeStart,\n                    confidence: overlapConfidence,\n                    crossCorrelation: crossCorr,\n                    spectralSimilarity: spectralSim,\n                    type: this.classifyOverlapType(window1, window2)\n                });\n            }\n        }\n        \n        // Merge nearby overlaps\n        return this.mergeNearbyOverlaps(overlaps);\n    }\n    \n    // Dynamic Ducking\n    configureDynamicDucking(primaryTrackId, secondaryTrackIds, options = {}) {\n        const config = {\n            ratio: options.ratio || this.config.duckingRatio,\n            attackTime: options.attackTime || 0.01,\n            releaseTime: options.releaseTime || 0.1,\n            threshold: options.threshold || -20,\n            lookahead: options.lookahead || 0.005\n        };\n        \n        const duckingConfig = {\n            primaryTrack: primaryTrackId,\n            secondaryTracks: secondaryTrackIds,\n            config,\n            active: true\n        };\n        \n        console.log('Dynamic ducking configured:', duckingConfig);\n        return duckingConfig;\n    }\n    \n    applyDynamicDucking(duckingConfig) {\n        const primaryTrackData = this.trackData.get(duckingConfig.primaryTrack);\n        if (!primaryTrackData) return false;\n        \n        const primaryBuffer = primaryTrackData.buffer.getChannelData(0);\n        const windowSize = 1024;\n        const duckingEvents = [];\n        \n        for (let i = 0; i < primaryBuffer.length - windowSize; i += windowSize / 4) {\n            const window = primaryBuffer.slice(i, i + windowSize);\n            const rms = this.calculateRMS(window);\n            const dbLevel = rms > 0 ? 20 * Math.log10(rms) : -Infinity;\n            \n            if (dbLevel > duckingConfig.config.threshold) {\n                const timeStart = i / primaryTrackData.buffer.sampleRate;\n                const timeEnd = (i + windowSize) / primaryTrackData.buffer.sampleRate;\n                \n                duckingEvents.push({\n                    startTime: timeStart,\n                    endTime: timeEnd,\n                    duckingAmount: duckingConfig.config.ratio,\n                    primaryLevel: dbLevel,\n                    secondaryTracks: duckingConfig.secondaryTracks\n                });\n            }\n        }\n        \n        // Apply ducking to secondary tracks\n        duckingConfig.secondaryTracks.forEach(trackId => {\n            this.applyDuckingToTrack(trackId, duckingEvents, duckingConfig.config);\n        });\n        \n        this.emit('duckingTriggered', {\n            primaryTrack: duckingConfig.primaryTrack,\n            secondaryTracks: duckingConfig.secondaryTracks,\n            events: duckingEvents\n        });\n        \n        return duckingEvents;\n    }\n    \n    applyDuckingToTrack(trackId, duckingEvents, config) {\n        const trackData = this.trackData.get(trackId);\n        if (!trackData) return;\n        \n        const buffer = trackData.buffer;\n        const channelData = buffer.getChannelData(0);\n        \n        // Create ducking envelope\n        const envelope = new Float32Array(channelData.length);\n        envelope.fill(1.0); // Default gain\n        \n        duckingEvents.forEach(event => {\n            const startSample = Math.floor(event.startTime * buffer.sampleRate);\n            const endSample = Math.floor(event.endTime * buffer.sampleRate);\n            const attackSamples = Math.floor(config.attackTime * buffer.sampleRate);\n            const releaseSamples = Math.floor(config.releaseTime * buffer.sampleRate);\n            \n            // Attack phase\n            for (let i = 0; i < attackSamples && (startSample + i) < envelope.length; i++) {\n                const gain = 1.0 - ((1.0 - config.ratio) * (i / attackSamples));\n                envelope[startSample + i] = Math.min(envelope[startSample + i], gain);\n            }\n            \n            // Sustain phase\n            for (let i = attackSamples; i < (endSample - startSample); i++) {\n                if ((startSample + i) < envelope.length) {\n                    envelope[startSample + i] = Math.min(envelope[startSample + i], config.ratio);\n                }\n            }\n            \n            // Release phase\n            for (let i = 0; i < releaseSamples && (endSample + i) < envelope.length; i++) {\n                const gain = config.ratio + ((1.0 - config.ratio) * (i / releaseSamples));\n                envelope[endSample + i] = Math.min(envelope[endSample + i], gain);\n            }\n        });\n        \n        // Store ducking envelope for later application\n        trackData.analysisData.duckingEnvelope = envelope;\n        \n        console.log(`Ducking envelope created for track ${trackId}`);\n    }\n    \n    // Utility Functions\n    calculateRMS(audioData) {\n        let sum = 0;\n        for (let i = 0; i < audioData.length; i++) {\n            sum += audioData[i] * audioData[i];\n        }\n        return Math.sqrt(sum / audioData.length);\n    }\n    \n    calculateZeroCrossingRate(audioData) {\n        let crossings = 0;\n        for (let i = 1; i < audioData.length; i++) {\n            if ((audioData[i] >= 0) !== (audioData[i - 1] >= 0)) {\n                crossings++;\n            }\n        }\n        return crossings / audioData.length;\n    }\n    \n    calculateSpectralCentroid(audioData) {\n        // Apply window function\n        const windowed = audioData.map((sample, i) => \n            sample * this.window[i % this.window.length]\n        );\n        \n        // Simple FFT approximation (in real implementation, use proper FFT)\n        let weightedSum = 0;\n        let magnitudeSum = 0;\n        \n        for (let i = 0; i < windowed.length / 2; i++) {\n            const magnitude = Math.abs(windowed[i]);\n            const frequency = i * (this.sampleRate / windowed.length);\n            \n            weightedSum += frequency * magnitude;\n            magnitudeSum += magnitude;\n        }\n        \n        return magnitudeSum > 0 ? weightedSum / magnitudeSum : 0;\n    }\n    \n    calculateCrossCorrelation(signal1, signal2) {\n        const length = Math.min(signal1.length, signal2.length);\n        let correlation = 0;\n        \n        for (let i = 0; i < length; i++) {\n            correlation += signal1[i] * signal2[i];\n        }\n        \n        return correlation / length;\n    }\n    \n    calculateSpectralSimilarity(signal1, signal2) {\n        // Simple spectral similarity based on frequency content\n        const fft1 = this.simpleFFT(signal1);\n        const fft2 = this.simpleFFT(signal2);\n        \n        let similarity = 0;\n        const bins = Math.min(fft1.length, fft2.length);\n        \n        for (let i = 0; i < bins; i++) {\n            similarity += Math.min(fft1[i], fft2[i]) / Math.max(fft1[i], fft2[i]);\n        }\n        \n        return similarity / bins;\n    }\n    \n    simpleFFT(signal) {\n        // Simplified FFT - in real implementation, use proper FFT library\n        const magnitudes = new Array(signal.length / 2);\n        \n        for (let k = 0; k < magnitudes.length; k++) {\n            let real = 0;\n            let imag = 0;\n            \n            for (let n = 0; n < signal.length; n++) {\n                const angle = -2 * Math.PI * k * n / signal.length;\n                real += signal[n] * Math.cos(angle);\n                imag += signal[n] * Math.sin(angle);\n            }\n            \n            magnitudes[k] = Math.sqrt(real * real + imag * imag);\n        }\n        \n        return magnitudes;\n    }\n    \n    findHarmonicPeaks(spectrum) {\n        const peaks = [];\n        const minPeakHeight = 0.1;\n        \n        for (let i = 1; i < spectrum.length - 1; i++) {\n            if (spectrum[i] > spectrum[i - 1] && \n                spectrum[i] > spectrum[i + 1] && \n                spectrum[i] > minPeakHeight) {\n                peaks.push({\n                    bin: i,\n                    frequency: i * (this.sampleRate / (spectrum.length * 2)),\n                    magnitude: spectrum[i]\n                });\n            }\n        }\n        \n        return peaks;\n    }\n    \n    createWindow(size, type) {\n        const window = new Float32Array(size);\n        \n        switch (type) {\n            case 'hanning':\n                for (let i = 0; i < size; i++) {\n                    window[i] = 0.5 * (1 - Math.cos(2 * Math.PI * i / (size - 1)));\n                }\n                break;\n            case 'hamming':\n                for (let i = 0; i < size; i++) {\n                    window[i] = 0.54 - 0.46 * Math.cos(2 * Math.PI * i / (size - 1));\n                }\n                break;\n            default: // rectangular\n                window.fill(1.0);\n        }\n        \n        return window;\n    }\n    \n    calculateSilenceConfidence(silenceRegions) {\n        if (silenceRegions.length === 0) return 0;\n        \n        const avgConfidence = silenceRegions.reduce((sum, region) => \n            sum + region.confidence, 0\n        ) / silenceRegions.length;\n        \n        return avgConfidence;\n    }\n    \n    mergeNearbyOverlaps(overlaps) {\n        if (overlaps.length <= 1) return overlaps;\n        \n        const merged = [];\n        let current = overlaps[0];\n        \n        for (let i = 1; i < overlaps.length; i++) {\n            const next = overlaps[i];\n            \n            if (next.startTime - current.endTime <= 0.1) { // 100ms tolerance\n                current.endTime = Math.max(current.endTime, next.endTime);\n                current.duration = current.endTime - current.startTime;\n                current.confidence = Math.max(current.confidence, next.confidence);\n            } else {\n                merged.push(current);\n                current = next;\n            }\n        }\n        \n        merged.push(current);\n        return merged;\n    }\n    \n    classifyOverlapType(signal1, signal2) {\n        // Classify the type of overlap (speech, music, effects)\n        const speechConf1 = this.mlModels.speechDetection.predict(this.simpleFFT(signal1));\n        const speechConf2 = this.mlModels.speechDetection.predict(this.simpleFFT(signal2));\n        const musicConf1 = this.mlModels.musicDetection.predict(this.simpleFFT(signal1));\n        const musicConf2 = this.mlModels.musicDetection.predict(this.simpleFFT(signal2));\n        \n        if (speechConf1 > 0.7 && speechConf2 > 0.7) {\n            return 'speech-speech';\n        } else if ((speechConf1 > 0.7 && musicConf2 > 0.7) || \n                   (musicConf1 > 0.7 && speechConf2 > 0.7)) {\n            return 'speech-music';\n        } else if (musicConf1 > 0.7 && musicConf2 > 0.7) {\n            return 'music-music';\n        } else {\n            return 'unknown';\n        }\n    }\n    \n    // Event System\n    on(event, handler) {\n        if (!this.eventHandlers[event]) {\n            this.eventHandlers[event] = [];\n        }\n        this.eventHandlers[event].push(handler);\n    }\n    \n    off(event, handler) {\n        if (this.eventHandlers[event]) {\n            const index = this.eventHandlers[event].indexOf(handler);\n            if (index > -1) {\n                this.eventHandlers[event].splice(index, 1);\n            }\n        }\n    }\n    \n    emit(event, data) {\n        if (this.eventHandlers[event]) {\n            this.eventHandlers[event].forEach(handler => {\n                try {\n                    handler(data);\n                } catch (error) {\n                    console.error(`Error in event handler for ${event}:`, error);\n                }\n            });\n        }\n    }\n    \n    // Status and Results\n    getAnalysisResults() {\n        return {\n            silence: this.analysisResults.silence,\n            overlaps: this.analysisResults.overlaps,\n            levels: this.analysisResults.levels,\n            spectral: this.analysisResults.spectral,\n            trackCount: this.trackData.size,\n            lastAnalysis: Math.max(...Array.from(this.trackData.values())\n                .map(track => track.lastAnalysis || 0))\n        };\n    }\n    \n    getTrackAnalysis(trackId) {\n        const trackData = this.trackData.get(trackId);\n        return trackData ? trackData.analysisData : null;\n    }\n    \n    // Cleanup\n    destroy() {\n        this.isAnalyzing = false;\n        \n        if (this.analyzeId) {\n            cancelAnimationFrame(this.analyzeId);\n        }\n        \n        this.analyserNodes.forEach(analyser => analyser.disconnect());\n        this.scriptProcessors.forEach(processor => processor.disconnect());\n        \n        this.trackData.clear();\n        this.analyserNodes.clear();\n        this.scriptProcessors.clear();\n        \n        console.log('Audio analyzer destroyed');\n    }\n}\n\nexport default AudioAnalyzer;"
