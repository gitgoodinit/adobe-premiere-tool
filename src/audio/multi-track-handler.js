/**
 * Multi-Track Audio Handler - Main Integration Layer
 * Coordinates all multi-track audio components and provides the public API
 * Supports up to 6 tracks with comprehensive audio processing capabilities
 */

import SubmixRouter from './submix-router.js';
import AudioAnalyzer from './audio-analyzer.js';
import MultiCamAudioAligner from './multicam-aligner.js';

class MultiTrackAudioHandler {
    constructor(options = {}) {
        // Initialize audio context
        this.audioContext = null;
        this.isInitialized = false;
        this.isDestroyed = false;
        
        // Configuration
        this.config = {
            maxTracks: options.maxTracks || 6,
            sampleRate: options.sampleRate || 44100,
            bufferSize: options.bufferSize || 512,
            enableWorklet: options.enableWorklet !== false,
            enableSubmixRouting: options.enableSubmixRouting !== false,
            enableAnalysis: options.enableAnalysis !== false,
            enableMultiCam: options.enableMultiCam !== false,
            workletPath: options.workletPath || './multi-track-processor.worklet.js'
        };
        
        // Core components
        this.submixRouter = null;
        this.audioAnalyzer = null;
        this.multiCamAligner = null;\n        this.audioWorklet = null;\n        \n        // Track management\n        this.tracks = new Map();\n        this.activeTrackCount = 0;\n        \n        // Processing state\n        this.isProcessing = false;\n        this.processingStartTime = null;\n        \n        // Event system\n        this.eventHandlers = {\n            'trackAdded': [],\n            'trackRemoved': [],\n            'trackStatusChanged': [],\n            'processingStarted': [],\n            'processingStopped': [],\n            'analysisComplete': [],\n            'syncDetected': [],\n            'error': []\n        };\n        \n        // Performance monitoring\n        this.performance = {\n            trackProcessingTimes: new Map(),\n            analysisTime: 0,\n            syncTime: 0,\n            totalProcessedSamples: 0,\n            startTime: Date.now()\n        };\n        \n        console.log('Multi-Track Audio Handler created with config:', this.config);\n    }\n    \n    // Initialization\n    async initialize() {\n        if (this.isInitialized) {\n            console.warn('Multi-Track Audio Handler already initialized');\n            return true;\n        }\n        \n        try {\n            console.log('Initializing Multi-Track Audio Handler...');\n            \n            // Initialize Web Audio Context\n            await this.initializeAudioContext();\n            \n            // Initialize core components\n            if (this.config.enableSubmixRouting) {\n                await this.initializeSubmixRouter();\n            }\n            \n            if (this.config.enableAnalysis) {\n                await this.initializeAudioAnalyzer();\n            }\n            \n            if (this.config.enableMultiCam) {\n                await this.initializeMultiCamAligner();\n            }\n            \n            // Initialize AudioWorklet for real-time processing\n            if (this.config.enableWorklet) {\n                await this.initializeAudioWorklet();\n            }\n            \n            // Setup event forwarding\n            this.setupEventForwarding();\n            \n            this.isInitialized = true;\n            console.log('Multi-Track Audio Handler initialized successfully');\n            \n            return true;\n            \n        } catch (error) {\n            console.error('Failed to initialize Multi-Track Audio Handler:', error);\n            this.emit('error', { type: 'initialization', error });\n            throw error;\n        }\n    }\n    \n    async initializeAudioContext() {\n        if (typeof AudioContext !== 'undefined') {\n            this.audioContext = new AudioContext({\n                sampleRate: this.config.sampleRate,\n                latencyHint: 'interactive'\n            });\n        } else if (typeof webkitAudioContext !== 'undefined') {\n            this.audioContext = new webkitAudioContext();\n        } else {\n            throw new Error('Web Audio API not supported');\n        }\n        \n        // Resume context if needed\n        if (this.audioContext.state === 'suspended') {\n            await this.audioContext.resume();\n        }\n        \n        console.log(`Audio context initialized: ${this.audioContext.sampleRate}Hz`);\n    }\n    \n    async initializeSubmixRouter() {\n        this.submixRouter = new SubmixRouter(this.audioContext, this.config.maxTracks);\n        await this.submixRouter.initializeRouting();\n        console.log('Submix router initialized');\n    }\n    \n    async initializeAudioAnalyzer() {\n        this.audioAnalyzer = new AudioAnalyzer(this.audioContext);\n        await this.audioAnalyzer.initializeAnalyzer();\n        console.log('Audio analyzer initialized');\n    }\n    \n    async initializeMultiCamAligner() {\n        this.multiCamAligner = new MultiCamAudioAligner(this.audioContext);\n        console.log('Multi-cam aligner initialized');\n    }\n    \n    async initializeAudioWorklet() {\n        try {\n            await this.audioContext.audioWorklet.addModule(this.config.workletPath);\n            \n            this.audioWorklet = new AudioWorkletNode(this.audioContext, 'multi-track-processor', {\n                numberOfInputs: this.config.maxTracks,\n                numberOfOutputs: 1,\n                outputChannelCount: [2], // Stereo output\n                processorOptions: {\n                    sampleRate: this.audioContext.sampleRate,\n                    bufferSize: this.config.bufferSize\n                }\n            });\n            \n            // Connect worklet to destination\n            this.audioWorklet.connect(this.audioContext.destination);\n            \n            // Setup worklet message handling\n            this.audioWorklet.port.onmessage = (event) => {\n                this.handleWorkletMessage(event.data);\n            };\n            \n            console.log('AudioWorklet initialized');\n            \n        } catch (error) {\n            console.warn('AudioWorklet initialization failed, using fallback:', error);\n            this.config.enableWorklet = false;\n        }\n    }\n    \n    setupEventForwarding() {\n        // Forward events from components\n        if (this.submixRouter) {\n            this.submixRouter.on('trackLevelChange', (data) => {\n                this.emit('trackStatusChanged', { ...data, source: 'submix' });\n            });\n            \n            this.submixRouter.on('routingChange', (data) => {\n                this.emit('trackStatusChanged', { ...data, source: 'routing' });\n            });\n        }\n        \n        if (this.audioAnalyzer) {\n            this.audioAnalyzer.on('silenceDetected', (data) => {\n                this.emit('analysisComplete', { type: 'silence', ...data });\n            });\n            \n            this.audioAnalyzer.on('overlapDetected', (data) => {\n                this.emit('analysisComplete', { type: 'overlap', ...data });\n            });\n        }\n        \n        if (this.multiCamAligner) {\n            this.multiCamAligner.on('syncDetected', (data) => {\n                this.emit('syncDetected', data);\n            });\n            \n            this.multiCamAligner.on('alignmentChanged', (data) => {\n                this.emit('trackStatusChanged', { ...data, source: 'alignment' });\n            });\n        }\n    }\n    \n    // Track Management\n    async addTrack(trackId, audioSource, options = {}) {\n        if (this.tracks.has(trackId)) {\n            throw new Error(`Track ${trackId} already exists`);\n        }\n        \n        if (this.tracks.size >= this.config.maxTracks) {\n            throw new Error(`Maximum track limit (${this.config.maxTracks}) reached`);\n        }\n        \n        console.log(`Adding track ${trackId}...`);\n        \n        const trackInfo = {\n            id: trackId,\n            name: options.name || `Track ${trackId}`,\n            type: options.type || 'audio', // 'audio', 'speech', 'music', 'effects'\n            audioSource,\n            audioBuffer: null,\n            submix: options.submix || 'main',\n            active: true,\n            muted: false,\n            soloed: false,\n            gain: options.gain || 1.0,\n            offset: options.offset || 0,\n            created: Date.now(),\n            lastProcessed: null,\n            metadata: options.metadata || {}\n        };\n        \n        // Load audio buffer if source is provided\n        if (audioSource) {\n            try {\n                trackInfo.audioBuffer = await this.loadAudioBuffer(audioSource);\n            } catch (error) {\n                console.error(`Failed to load audio for track ${trackId}:`, error);\n                throw error;\n            }\n        }\n        \n        this.tracks.set(trackId, trackInfo);\n        this.activeTrackCount++;\n        \n        // Register with components\n        await this.registerTrackWithComponents(trackId, trackInfo);\n        \n        this.emit('trackAdded', { trackId, trackInfo });\n        \n        console.log(`Track ${trackId} added successfully`);\n        return trackId;\n    }\n    \n    async registerTrackWithComponents(trackId, trackInfo) {\n        // Register with submix router\n        if (this.submixRouter && trackInfo.audioBuffer) {\n            const audioSource = this.audioContext.createBufferSource();\n            audioSource.buffer = trackInfo.audioBuffer;\n            \n            this.submixRouter.connectTrack(trackId, audioSource);\n            this.submixRouter.routeTrackToSubmix(trackId, trackInfo.submix);\n            this.submixRouter.setTrackGain(trackId, trackInfo.gain);\n        }\n        \n        // Register with audio analyzer\n        if (this.audioAnalyzer && trackInfo.audioBuffer) {\n            this.audioAnalyzer.addTrack(trackId, trackInfo.audioBuffer);\n        }\n        \n        // Register with multi-cam aligner if it's a camera track\n        if (this.multiCamAligner && trackInfo.type === 'camera') {\n            this.multiCamAligner.addCamera(trackId, {\n                name: trackInfo.name,\n                frameRate: trackInfo.metadata.frameRate,\n                timecode: trackInfo.metadata.timecode\n            });\n            \n            if (trackInfo.audioBuffer) {\n                this.multiCamAligner.loadAudioForCamera(trackId, trackInfo.audioBuffer);\n            }\n        }\n        \n        // Configure worklet\n        if (this.audioWorklet) {\n            this.audioWorklet.port.postMessage({\n                type: 'setTrackActive',\n                data: { trackId, active: true }\n            });\n            \n            this.audioWorklet.port.postMessage({\n                type: 'setSubmixRouting',\n                data: { trackId, submix: trackInfo.submix }\n            });\n        }\n    }\n    \n    removeTrack(trackId) {\n        const track = this.tracks.get(trackId);\n        if (!track) {\n            console.warn(`Track ${trackId} not found`);\n            return false;\n        }\n        \n        console.log(`Removing track ${trackId}...`);\n        \n        // Unregister from components\n        if (this.submixRouter) {\n            this.submixRouter.disconnectTrack(trackId);\n        }\n        \n        if (this.audioAnalyzer) {\n            this.audioAnalyzer.removeTrack(trackId);\n        }\n        \n        if (this.multiCamAligner) {\n            this.multiCamAligner.removeCamera(trackId);\n        }\n        \n        if (this.audioWorklet) {\n            this.audioWorklet.port.postMessage({\n                type: 'setTrackActive',\n                data: { trackId, active: false }\n            });\n        }\n        \n        // Remove from tracks\n        this.tracks.delete(trackId);\n        this.activeTrackCount--;\n        \n        this.emit('trackRemoved', { trackId });\n        \n        console.log(`Track ${trackId} removed successfully`);\n        return true;\n    }\n    \n    // Audio Processing Operations\n    async runSilenceDetection(trackIds = null, options = {}) {\n        if (!this.audioAnalyzer) {\n            throw new Error('Audio analyzer not initialized');\n        }\n        \n        const targetTracks = trackIds || Array.from(this.tracks.keys());\n        const results = new Map();\n        \n        console.log(`Running silence detection on ${targetTracks.length} tracks...`);\n        \n        const startTime = performance.now();\n        \n        for (const trackId of targetTracks) {\n            if (this.tracks.has(trackId)) {\n                try {\n                    const silenceRegions = await this.audioAnalyzer.detectSilence(trackId, options);\n                    results.set(trackId, silenceRegions);\n                } catch (error) {\n                    console.error(`Silence detection failed for track ${trackId}:`, error);\n                    results.set(trackId, []);\n                }\n            }\n        }\n        \n        const analysisTime = performance.now() - startTime;\n        this.performance.analysisTime += analysisTime;\n        \n        console.log(`Silence detection completed in ${analysisTime.toFixed(2)}ms`);\n        \n        return results;\n    }\n    \n    async runOverlapDetection(trackIds = null, options = {}) {\n        if (!this.audioAnalyzer) {\n            throw new Error('Audio analyzer not initialized');\n        }\n        \n        const targetTracks = trackIds || Array.from(this.tracks.keys());\n        \n        if (targetTracks.length < 2) {\n            throw new Error('Overlap detection requires at least 2 tracks');\n        }\n        \n        console.log(`Running overlap detection on ${targetTracks.length} tracks...`);\n        \n        const startTime = performance.now();\n        const overlaps = await this.audioAnalyzer.detectOverlaps(targetTracks, options);\n        const analysisTime = performance.now() - startTime;\n        \n        this.performance.analysisTime += analysisTime;\n        \n        console.log(`Overlap detection completed in ${analysisTime.toFixed(2)}ms`);\n        console.log(`Found ${overlaps.length} overlapping regions`);\n        \n        return overlaps;\n    }\n    \n    calculateAutoTrimPoints(trackId, options = {}) {\n        if (!this.audioAnalyzer) {\n            throw new Error('Audio analyzer not initialized');\n        }\n        \n        const track = this.tracks.get(trackId);\n        if (!track) {\n            throw new Error(`Track ${trackId} not found`);\n        }\n        \n        return this.audioAnalyzer.calculateAutoTrimPoints(trackId, options);\n    }\n    \n    setupDynamicDucking(primaryTrackId, secondaryTrackIds, options = {}) {\n        if (!this.audioAnalyzer) {\n            throw new Error('Audio analyzer not initialized');\n        }\n        \n        const duckingConfig = this.audioAnalyzer.configureDynamicDucking(\n            primaryTrackId, \n            secondaryTrackIds, \n            options\n        );\n        \n        // Apply to worklet if available\n        if (this.audioWorklet) {\n            this.audioWorklet.port.postMessage({\n                type: 'enableDucking',\n                data: {\n                    primaryTrack: primaryTrackId,\n                    secondaryTracks: secondaryTrackIds\n                }\n            });\n        }\n        \n        // Apply to submix router\n        if (this.submixRouter) {\n            // Configure ducking at submix level\n            secondaryTrackIds.forEach(trackId => {\n                const track = this.tracks.get(trackId);\n                if (track) {\n                    this.submixRouter.applyPluginWithWorkaround(trackId, 'compressor', {\n                        threshold: options.threshold || -20,\n                        ratio: options.ratio || 4,\n                        attack: options.attackTime || 0.01,\n                        release: options.releaseTime || 0.1\n                    });\n                }\n            });\n        }\n        \n        return duckingConfig;\n    }\n    \n    // Multi-Camera Sync\n    async createSyncGroup(trackIds, options = {}) {\n        if (!this.multiCamAligner) {\n            throw new Error('Multi-cam aligner not initialized');\n        }\n        \n        // Ensure all tracks are camera tracks\n        const cameraTracks = trackIds.filter(trackId => {\n            const track = this.tracks.get(trackId);\n            return track && (track.type === 'camera' || options.forceCameraType);\n        });\n        \n        if (cameraTracks.length < 2) {\n            throw new Error('Sync group requires at least 2 camera tracks');\n        }\n        \n        return this.multiCamAligner.createSyncGroup(cameraTracks, options);\n    }\n    \n    async detectCameraSync(groupId, options = {}) {\n        if (!this.multiCamAligner) {\n            throw new Error('Multi-cam aligner not initialized');\n        }\n        \n        console.log(`Detecting sync for group ${groupId}...`);\n        \n        const startTime = performance.now();\n        const results = await this.multiCamAligner.detectSync(groupId, options);\n        const syncTime = performance.now() - startTime;\n        \n        this.performance.syncTime += syncTime;\n        \n        console.log(`Camera sync detection completed in ${syncTime.toFixed(2)}ms`);\n        \n        return results;\n    }\n    \n    setManualCameraOffset(trackId, offsetSeconds) {\n        if (!this.multiCamAligner) {\n            throw new Error('Multi-cam aligner not initialized');\n        }\n        \n        return this.multiCamAligner.setManualOffset(trackId, offsetSeconds);\n    }\n    \n    // Submix Control\n    routeTrackToSubmix(trackId, submixName) {\n        const track = this.tracks.get(trackId);\n        if (!track) {\n            throw new Error(`Track ${trackId} not found`);\n        }\n        \n        track.submix = submixName;\n        \n        if (this.submixRouter) {\n            this.submixRouter.routeTrackToSubmix(trackId, submixName);\n        }\n        \n        if (this.audioWorklet) {\n            this.audioWorklet.port.postMessage({\n                type: 'setSubmixRouting',\n                data: { trackId, submix: submixName }\n            });\n        }\n        \n        this.emit('trackStatusChanged', { \n            trackId, \n            submix: submixName, \n            source: 'routing' \n        });\n        \n        return true;\n    }\n    \n    setSubmixGain(submixName, gain) {\n        if (this.submixRouter) {\n            return this.submixRouter.setSubmixGain(submixName, gain);\n        }\n        return false;\n    }\n    \n    setTrackGain(trackId, gain) {\n        const track = this.tracks.get(trackId);\n        if (!track) {\n            return false;\n        }\n        \n        track.gain = gain;\n        \n        if (this.submixRouter) {\n            this.submixRouter.setTrackGain(trackId, gain);\n        }\n        \n        this.emit('trackStatusChanged', { \n            trackId, \n            gain, \n            source: 'gain' \n        });\n        \n        return true;\n    }\n    \n    muteTrack(trackId, muted = true) {\n        const track = this.tracks.get(trackId);\n        if (!track) {\n            return false;\n        }\n        \n        track.muted = muted;\n        \n        if (this.submixRouter) {\n            this.submixRouter.muteTrack(trackId, muted);\n        }\n        \n        this.emit('trackStatusChanged', { \n            trackId, \n            muted, \n            source: 'mute' \n        });\n        \n        return true;\n    }\n    \n    soloTrack(trackId, soloed = true) {\n        const track = this.tracks.get(trackId);\n        if (!track) {\n            return false;\n        }\n        \n        track.soloed = soloed;\n        \n        if (this.submixRouter) {\n            this.submixRouter.soloTrack(trackId, soloed);\n        }\n        \n        this.emit('trackStatusChanged', { \n            trackId, \n            soloed, \n            source: 'solo' \n        });\n        \n        return true;\n    }\n    \n    // Processing Control\n    startProcessing() {\n        if (this.isProcessing) {\n            console.warn('Processing already started');\n            return false;\n        }\n        \n        console.log('Starting multi-track processing...');\n        \n        this.isProcessing = true;\n        this.processingStartTime = performance.now();\n        \n        // Start level monitoring\n        if (this.submixRouter) {\n            this.submixRouter.startLevelMonitoring();\n        }\n        \n        // Configure worklet for processing\n        if (this.audioWorklet) {\n            this.audioWorklet.port.postMessage({\n                type: 'configure',\n                data: {\n                    silenceThreshold: -40,\n                    overlapThreshold: 0.3,\n                    duckingRatio: 0.3\n                }\n            });\n        }\n        \n        this.emit('processingStarted', { \n            timestamp: this.processingStartTime,\n            activeTrackCount: this.activeTrackCount\n        });\n        \n        return true;\n    }\n    \n    stopProcessing() {\n        if (!this.isProcessing) {\n            console.warn('Processing not started');\n            return false;\n        }\n        \n        console.log('Stopping multi-track processing...');\n        \n        this.isProcessing = false;\n        const processingDuration = performance.now() - this.processingStartTime;\n        \n        // Stop level monitoring\n        if (this.submixRouter) {\n            this.submixRouter.stopLevelMonitoring();\n        }\n        \n        this.emit('processingStopped', { \n            duration: processingDuration,\n            totalProcessedSamples: this.performance.totalProcessedSamples\n        });\n        \n        return true;\n    }\n    \n    // Utility Functions\n    async loadAudioBuffer(audioSource) {\n        if (audioSource instanceof AudioBuffer) {\n            return audioSource;\n        }\n        \n        if (audioSource instanceof ArrayBuffer) {\n            return await this.audioContext.decodeAudioData(audioSource);\n        }\n        \n        if (typeof audioSource === 'string') {\n            // Load from URL\n            const response = await fetch(audioSource);\n            const arrayBuffer = await response.arrayBuffer();\n            return await this.audioContext.decodeAudioData(arrayBuffer);\n        }\n        \n        throw new Error('Unsupported audio source type');\n    }\n    \n    handleWorkletMessage(message) {\n        const { type, data } = message;\n        \n        switch (type) {\n            case 'periodicUpdate':\n                this.performance.totalProcessedSamples = data.processedSamples;\n                this.emit('trackStatusChanged', {\n                    source: 'worklet',\n                    ...data\n                });\n                break;\n                \n            case 'silenceDetected':\n                this.emit('analysisComplete', {\n                    type: 'silence',\n                    source: 'worklet',\n                    ...data\n                });\n                break;\n                \n            case 'overlapDetected':\n                this.emit('analysisComplete', {\n                    type: 'overlap',\n                    source: 'worklet',\n                    ...data\n                });\n                break;\n                \n            case 'duckingTriggered':\n                this.emit('analysisComplete', {\n                    type: 'ducking',\n                    source: 'worklet',\n                    ...data\n                });\n                break;\n                \n            default:\n                console.log('Unknown worklet message:', type, data);\n        }\n    }\n    \n    // Status and Information\n    getStatus() {\n        const status = {\n            isInitialized: this.isInitialized,\n            isProcessing: this.isProcessing,\n            activeTrackCount: this.activeTrackCount,\n            maxTracks: this.config.maxTracks,\n            config: this.config,\n            performance: {\n                ...this.performance,\n                uptime: Date.now() - this.performance.startTime\n            },\n            tracks: {},\n            submixRouter: this.submixRouter?.getStatus() || null,\n            audioAnalyzer: this.audioAnalyzer?.getAnalysisResults() || null,\n            multiCamAligner: this.multiCamAligner?.getAlignmentStatus() || null\n        };\n        \n        // Add track information\n        this.tracks.forEach((track, trackId) => {\n            status.tracks[trackId] = {\n                id: trackId,\n                name: track.name,\n                type: track.type,\n                submix: track.submix,\n                active: track.active,\n                muted: track.muted,\n                soloed: track.soloed,\n                gain: track.gain,\n                offset: track.offset,\n                hasAudio: !!track.audioBuffer,\n                duration: track.audioBuffer?.duration || 0\n            };\n        });\n        \n        return status;\n    }\n    \n    getTrackInfo(trackId) {\n        const track = this.tracks.get(trackId);\n        if (!track) {\n            return null;\n        }\n        \n        return {\n            ...track,\n            submixInfo: this.submixRouter?.getTrackInfo(trackId) || null,\n            analysisInfo: this.audioAnalyzer?.getTrackAnalysis(trackId) || null,\n            alignmentInfo: this.multiCamAligner?.getAlignmentStatus(trackId) || null\n        };\n    }\n    \n    exportConfiguration() {\n        const config = {\n            timestamp: Date.now(),\n            version: '1.0.0',\n            config: this.config,\n            tracks: {},\n            submixRouting: this.submixRouter?.getStatus() || null,\n            multiCamAlignment: this.multiCamAligner?.exportAlignment() || null\n        };\n        \n        this.tracks.forEach((track, trackId) => {\n            config.tracks[trackId] = {\n                name: track.name,\n                type: track.type,\n                submix: track.submix,\n                gain: track.gain,\n                offset: track.offset,\n                metadata: track.metadata\n            };\n        });\n        \n        return config;\n    }\n    \n    async importConfiguration(configData) {\n        try {\n            console.log('Importing configuration...');\n            \n            // Clear existing tracks\n            const existingTracks = Array.from(this.tracks.keys());\n            existingTracks.forEach(trackId => this.removeTrack(trackId));\n            \n            // Import tracks\n            for (const [trackId, trackConfig] of Object.entries(configData.tracks)) {\n                await this.addTrack(parseInt(trackId), null, {\n                    name: trackConfig.name,\n                    type: trackConfig.type,\n                    submix: trackConfig.submix,\n                    gain: trackConfig.gain,\n                    offset: trackConfig.offset,\n                    metadata: trackConfig.metadata\n                });\n            }\n            \n            // Import multi-cam alignment if available\n            if (this.multiCamAligner && configData.multiCamAlignment) {\n                this.multiCamAligner.importAlignment(configData.multiCamAlignment);\n            }\n            \n            console.log('Configuration imported successfully');\n            return true;\n            \n        } catch (error) {\n            console.error('Failed to import configuration:', error);\n            throw error;\n        }\n    }\n    \n    // Event System\n    on(event, handler) {\n        if (!this.eventHandlers[event]) {\n            this.eventHandlers[event] = [];\n        }\n        this.eventHandlers[event].push(handler);\n    }\n    \n    off(event, handler) {\n        if (this.eventHandlers[event]) {\n            const index = this.eventHandlers[event].indexOf(handler);\n            if (index > -1) {\n                this.eventHandlers[event].splice(index, 1);\n            }\n        }\n    }\n    \n    emit(event, data) {\n        if (this.eventHandlers[event]) {\n            this.eventHandlers[event].forEach(handler => {\n                try {\n                    handler(data);\n                } catch (error) {\n                    console.error(`Error in event handler for ${event}:`, error);\n                }\n            });\n        }\n    }\n    \n    // Cleanup\n    async destroy() {\n        if (this.isDestroyed) {\n            return;\n        }\n        \n        console.log('Destroying Multi-Track Audio Handler...');\n        \n        this.stopProcessing();\n        \n        // Remove all tracks\n        const trackIds = Array.from(this.tracks.keys());\n        trackIds.forEach(trackId => this.removeTrack(trackId));\n        \n        // Destroy components\n        if (this.submixRouter) {\n            this.submixRouter.destroy();\n            this.submixRouter = null;\n        }\n        \n        if (this.audioAnalyzer) {\n            this.audioAnalyzer.destroy();\n            this.audioAnalyzer = null;\n        }\n        \n        if (this.multiCamAligner) {\n            this.multiCamAligner.destroy();\n            this.multiCamAligner = null;\n        }\n        \n        if (this.audioWorklet) {\n            this.audioWorklet.disconnect();\n            this.audioWorklet = null;\n        }\n        \n        if (this.audioContext && this.audioContext.state !== 'closed') {\n            await this.audioContext.close();\n            this.audioContext = null;\n        }\n        \n        this.tracks.clear();\n        this.eventHandlers = {};\n        this.isDestroyed = true;\n        \n        console.log('Multi-Track Audio Handler destroyed');\n    }\n}\n\nexport default MultiTrackAudioHandler;"
